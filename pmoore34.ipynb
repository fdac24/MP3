{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMgEZMw/9Uxr9tAz/tjvsCd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/caderbug/MP3/blob/main/pmoore34.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install urlextract\n",
        "!pip install requests"
      ],
      "metadata": {
        "id": "TCVCohm35Y2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xtg44eTR1Xax"
      },
      "outputs": [],
      "source": [
        "import json, re, gzip\n",
        "import requests\n",
        "from urlextract import URLExtract\n",
        "import sys, gzip\n",
        "\n",
        "\n",
        "utid = 'pmoore34'\n",
        "base= { 'model':'https://huggingface.co/', 'data': 'https://huggingface.co/datasets/', 'source': 'https://' }\n",
        "post = '/raw/main/README.md'\n",
        "postGH = 'blob/master/README.md'\n",
        "\n",
        "extU = URLExtract()\n",
        "DOIpattern = r'\\b(10\\.\\d{4,9}\\/[-._;()/:A-Z0-9]+)\\b/i'\n",
        "\n",
        "\n",
        "def extractURLs (c):\n",
        " res = extU.find_urls (c)\n",
        " return res\n",
        "\n",
        "def extractDOIs (c):\n",
        " res = re.findall (DOIpattern, c)\n",
        " return res\n",
        "\n",
        "fo = gzip.open(f\"output/{utid}.json.gz\", 'w')\n",
        "\n",
        "def run (tp):\n",
        " post0 = post\n",
        " with open(f\"input/{utid}_{tp}\", 'r') as f:\n",
        "  for line in f:\n",
        "   line = line.strip ()\n",
        "   if tp == 'source':\n",
        "    (npapers,line) = line.split(';');\n",
        "    post0 = postGH\n",
        "   print(line)\n",
        "   url = base[tp] + f\"{line}{post0}\"\n",
        "   r = requests.get (url)\n",
        "   content = r.text;\n",
        "   #github returns repos that do not exist, need to detect that here\n",
        "   #github when you give master instead of main, that might cause issues as well\n",
        "   urls = extractURLs(content)\n",
        "   dois = extractDOIs(content)\n",
        "   res = { 'ID': line, 'type': tp, 'url': url, 'content': content, 'links': urls, 'dois': dois }\n",
        "   out = json.dumps(res, ensure_ascii=False)\n",
        "   fo.write((out+\"\\n\").encode())\n",
        "\n",
        "with gzip.open(f\"output/{utid}.json.gz\", 'wt', encoding='utf-8') as fo:\n",
        "    for tp in ['model', 'data', 'source']:\n",
        "        for entry in process_type(tp):\n",
        "            json.dump(entry, fo)\n",
        "            fo.write('\\n')\n",
        "run('model')\n",
        "run('data')\n",
        "run('source')"
      ]
    }
  ]
}